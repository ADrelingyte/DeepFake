{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBdLjKfwgoJv",
        "outputId": "8ded01de-baaf-4b19-e78a-4710ddd514c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attempt to Train Polish dataset (100 samples in directory)\n",
        "\n",
        "Unsuccesful: stuck on epoch 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IoNtX_eyuQRv",
        "outputId": "9487922d-7264-4ce6-9fb7-1922c761b6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > Setting up Audio Processor...\n",
            " | > sample_rate:24000\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:1024\n",
            " | > power:1.5\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:11025\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:1.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:True\n",
            " | > trim_db:60.0\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:2.718281828459045\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " | > Found 100 files in /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir\n",
            " > Init speaker_embedding layer.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " > Training Environment:\n",
            " | > Backend: Torch\n",
            " | > Mixed precision: False\n",
            " | > Precision: float32\n",
            " | > Current device: 0\n",
            " | > Num. of GPUs: 1\n",
            " | > Num. of CPUs: 2\n",
            " | > Num. of Torch Threads: 1\n",
            " | > Torch seed: 54321\n",
            " | > Torch CUDNN: True\n",
            " | > Torch CUDNN deterministic: False\n",
            " | > Torch CUDNN benchmark: False\n",
            " | > Torch TF32 MatMul: False\n",
            " > Start Tensorboard: tensorboard --logdir=/content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000\n",
            "\n",
            " > Model has 34615866 parameters\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > `speakers.pth` is saved to /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000/speakers.pth.\n",
            " > `speakers_file` is updated in the config.json.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/25\u001b[0m\n",
            " --> /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-11-01 17:21:29) \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: pl\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 99\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 250\n",
            " | > Min text length: 115\n",
            " | > Avg text length: 183.6161616161616\n",
            " | \n",
            " | > Max audio length: 358822.0\n",
            " | > Min audio length: 240022.0\n",
            " | > Avg audio length: 294283.83838383836\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m   --> TIME: 2023-11-01 17:21:40 -- STEP: 0/4 -- GLOBAL_STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 39.23311233520508  (39.23311233520508)\n",
            "     | > postnet_loss: 41.16348648071289  (41.16348648071289)\n",
            "     | > capaciton_reconstruction_loss: 474607.15625  (474607.15625)\n",
            "     | > capacitron_vae_loss: -0.0005424190312623978  (-0.0005424190312623978)\n",
            "     | > capacitron_vae_beta_loss: 45.73677062988281  (45.73677062988281)\n",
            "     | > capacitron_vae_kl_term: 4.263230800628662  (4.263230800628662)\n",
            "     | > capacitron_beta: 1.0  (1.0)\n",
            "     | > stopnet_loss: 0.7341765761375427  (0.7341765761375427)\n",
            "     | > loss: 81.13023376464844  (81.13023376464844)\n",
            "     | > align_error: 0.9730942882597446  (0.9730942882597446)\n",
            "     | > grad_norm: 0  (0)\n",
            "     | > current_lr: 0.001 \n",
            "     | > step_time: 5.3831  (5.383062124252319)\n",
            "     | > loader_time: 5.7199  (5.71993088722229)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: audio amplitude out of range, auto clipped.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: pl\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 1\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 244\n",
            " | > Min text length: 244\n",
            " | > Avg text length: 244.0\n",
            " | \n",
            " | > Max audio length: 348502.0\n",
            " | > Min audio length: 348502.0\n",
            " | > Avg audio length: 348502.0\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 40.663272857666016  (40.663272857666016)\n",
            "     | > postnet_loss: 40.65840148925781  (40.65840148925781)\n",
            "     | > capaciton_reconstruction_loss: 620206.625  (620206.625)\n",
            "     | > capacitron_vae_loss: -0.0004353635595180094  (-0.0004353635595180094)\n",
            "     | > capacitron_vae_beta_loss: 45.69575881958008  (45.69575881958008)\n",
            "     | > capacitron_vae_kl_term: 4.228603363037109  (4.228603363037109)\n",
            "     | > capacitron_beta: 0.9983475208282471  (0.9983475208282471)\n",
            "     | > stopnet_loss: 0.6581758856773376  (0.6581758856773376)\n",
            "     | > loss: 81.97940826416016  (81.97940826416016)\n",
            "     | > align_error: 0.9743180014193058  (0.9743180014193058)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: audio amplitude out of range, auto clipped.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.41062259674072266 \u001b[0m(+0)\n",
            "     | > avg_decoder_loss: 40.663272857666016 \u001b[0m(+0)\n",
            "     | > avg_postnet_loss: 40.65840148925781 \u001b[0m(+0)\n",
            "     | > avg_capaciton_reconstruction_loss: 620206.625 \u001b[0m(+0)\n",
            "     | > avg_capacitron_vae_loss: -0.0004353635595180094 \u001b[0m(+0)\n",
            "     | > avg_capacitron_vae_beta_loss: 45.69575881958008 \u001b[0m(+0)\n",
            "     | > avg_capacitron_vae_kl_term: 4.228603363037109 \u001b[0m(+0)\n",
            "     | > avg_capacitron_beta: 0.9983475208282471 \u001b[0m(+0)\n",
            "     | > avg_stopnet_loss: 0.6581758856773376 \u001b[0m(+0)\n",
            "     | > avg_loss: 81.97940826416016 \u001b[0m(+0)\n",
            "     | > avg_align_error: 0.9743180014193058 \u001b[0m(+0)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000/best_model_4.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/25\u001b[0m\n",
            " --> /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-11-01 17:22:17) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 39.26753616333008  (39.26753616333008)\n",
            "     | > postnet_loss: 38.72310256958008  (38.72310256958008)\n",
            "     | > capaciton_reconstruction_loss: 608111.3125  (608111.3125)\n",
            "     | > capacitron_vae_loss: -0.000433931068982929  (-0.000433931068982929)\n",
            "     | > capacitron_vae_beta_loss: 45.545406341552734  (45.545406341552734)\n",
            "     | > capacitron_vae_kl_term: 4.214628219604492  (4.214628219604492)\n",
            "     | > capacitron_beta: 0.9947589635848999  (0.9947589635848999)\n",
            "     | > stopnet_loss: 0.7441926598548889  (0.7441926598548889)\n",
            "     | > loss: 78.7343978881836  (78.7343978881836)\n",
            "     | > align_error: 0.9753244500607252  (0.9753244500607252)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.8127584457397461 \u001b[0m(+0.40213584899902344)\n",
            "     | > avg_decoder_loss:\u001b[92m 39.26753616333008 \u001b[0m(-1.3957366943359375)\n",
            "     | > avg_postnet_loss:\u001b[92m 38.72310256958008 \u001b[0m(-1.9352989196777344)\n",
            "     | > avg_capaciton_reconstruction_loss:\u001b[92m 608111.3125 \u001b[0m(-12095.3125)\n",
            "     | > avg_capacitron_vae_loss:\u001b[91m -0.000433931068982929 \u001b[0m(+1.4324905350804329e-06)\n",
            "     | > avg_capacitron_vae_beta_loss:\u001b[92m 45.545406341552734 \u001b[0m(-0.15035247802734375)\n",
            "     | > avg_capacitron_vae_kl_term:\u001b[92m 4.214628219604492 \u001b[0m(-0.013975143432617188)\n",
            "     | > avg_capacitron_beta:\u001b[92m 0.9947589635848999 \u001b[0m(-0.003588557243347168)\n",
            "     | > avg_stopnet_loss:\u001b[91m 0.7441926598548889 \u001b[0m(+0.08601677417755127)\n",
            "     | > avg_loss:\u001b[92m 78.7343978881836 \u001b[0m(-3.2450103759765625)\n",
            "     | > avg_align_error:\u001b[91m 0.9753244500607252 \u001b[0m(+0.0010064486414194107)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000/best_model_8.pth\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: audio amplitude out of range, auto clipped.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 2/25\u001b[0m\n",
            " --> /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-11-01 17:22:59) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 35.80514144897461  (35.80514144897461)\n",
            "     | > postnet_loss: 27.7950496673584  (27.7950496673584)\n",
            "     | > capaciton_reconstruction_loss: 578298.8125  (578298.8125)\n",
            "     | > capacitron_vae_loss: -0.00043189729331061244  (-0.00043189729331061244)\n",
            "     | > capacitron_vae_beta_loss: 45.331939697265625  (45.331939697265625)\n",
            "     | > capacitron_vae_kl_term: 4.206365585327148  (4.206365585327148)\n",
            "     | > capacitron_beta: 0.9899179935455322  (0.9899179935455322)\n",
            "     | > stopnet_loss: 0.4913251996040344  (0.4913251996040344)\n",
            "     | > loss: 64.09107971191406  (64.09107971191406)\n",
            "     | > align_error: 0.9776151347905397  (0.9776151347905397)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.7110562324523926 \u001b[0m(-0.10170221328735352)\n",
            "     | > avg_decoder_loss:\u001b[92m 35.80514144897461 \u001b[0m(-3.4623947143554688)\n",
            "     | > avg_postnet_loss:\u001b[92m 27.7950496673584 \u001b[0m(-10.92805290222168)\n",
            "     | > avg_capaciton_reconstruction_loss:\u001b[92m 578298.8125 \u001b[0m(-29812.5)\n",
            "     | > avg_capacitron_vae_loss:\u001b[91m -0.00043189729331061244 \u001b[0m(+2.033775672316551e-06)\n",
            "     | > avg_capacitron_vae_beta_loss:\u001b[92m 45.331939697265625 \u001b[0m(-0.21346664428710938)\n",
            "     | > avg_capacitron_vae_kl_term:\u001b[92m 4.206365585327148 \u001b[0m(-0.00826263427734375)\n",
            "     | > avg_capacitron_beta:\u001b[92m 0.9899179935455322 \u001b[0m(-0.004840970039367676)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.4913251996040344 \u001b[0m(-0.2528674602508545)\n",
            "     | > avg_loss:\u001b[92m 64.09107971191406 \u001b[0m(-14.643318176269531)\n",
            "     | > avg_align_error:\u001b[91m 0.9776151347905397 \u001b[0m(+0.0022906847298145294)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000/best_model_12.pth\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: audio amplitude out of range, auto clipped.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 3/25\u001b[0m\n",
            " --> /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-11-01 17:23:46) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 29.225866317749023  (29.225866317749023)\n",
            "     | > postnet_loss: 11.129799842834473  (11.129799842834473)\n",
            "     | > capaciton_reconstruction_loss: 511884.1875  (511884.1875)\n",
            "     | > capacitron_vae_loss: -0.00042898187530227005  (-0.00042898187530227005)\n",
            "     | > capacitron_vae_beta_loss: 45.025936126708984  (45.025936126708984)\n",
            "     | > capacitron_vae_kl_term: 4.25537109375  (4.25537109375)\n",
            "     | > capacitron_beta: 0.9842890501022339  (0.9842890501022339)\n",
            "     | > stopnet_loss: 0.3188757300376892  (0.3188757300376892)\n",
            "     | > loss: 40.67411422729492  (40.67411422729492)\n",
            "     | > align_error: 0.9806569013744593  (0.9806569013744593)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.641592264175415 \u001b[0m(-0.06946396827697754)\n",
            "     | > avg_decoder_loss:\u001b[92m 29.225866317749023 \u001b[0m(-6.579275131225586)\n",
            "     | > avg_postnet_loss:\u001b[92m 11.129799842834473 \u001b[0m(-16.665249824523926)\n",
            "     | > avg_capaciton_reconstruction_loss:\u001b[92m 511884.1875 \u001b[0m(-66414.625)\n",
            "     | > avg_capacitron_vae_loss:\u001b[91m -0.00042898187530227005 \u001b[0m(+2.9154180083423853e-06)\n",
            "     | > avg_capacitron_vae_beta_loss:\u001b[92m 45.025936126708984 \u001b[0m(-0.3060035705566406)\n",
            "     | > avg_capacitron_vae_kl_term:\u001b[91m 4.25537109375 \u001b[0m(+0.04900550842285156)\n",
            "     | > avg_capacitron_beta:\u001b[92m 0.9842890501022339 \u001b[0m(-0.00562894344329834)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.3188757300376892 \u001b[0m(-0.17244946956634521)\n",
            "     | > avg_loss:\u001b[92m 40.67411422729492 \u001b[0m(-23.41696548461914)\n",
            "     | > avg_align_error:\u001b[91m 0.9806569013744593 \u001b[0m(+0.003041766583919525)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000/best_model_16.pth\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: audio amplitude out of range, auto clipped.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 4/25\u001b[0m\n",
            " --> /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-11-01 17:24:33) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 20.429006576538086  (20.429006576538086)\n",
            "     | > postnet_loss: 5.5633416175842285  (5.5633416175842285)\n",
            "     | > capaciton_reconstruction_loss: 413216.125  (413216.125)\n",
            "     | > capacitron_vae_loss: -0.00042412534821778536  (-0.00042412534821778536)\n",
            "     | > capacitron_vae_beta_loss: 44.516197204589844  (44.516197204589844)\n",
            "     | > capacitron_vae_kl_term: 4.491630554199219  (4.491630554199219)\n",
            "     | > capacitron_beta: 0.9781980514526367  (0.9781980514526367)\n",
            "     | > stopnet_loss: 0.15812799334526062  (0.15812799334526062)\n",
            "     | > loss: 26.150053024291992  (26.150053024291992)\n",
            "     | > align_error: 0.9835020024329424  (0.9835020024329424)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: audio amplitude out of range, auto clipped.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.6665661334991455 \u001b[0m(+0.02497386932373047)\n",
            "     | > avg_decoder_loss:\u001b[92m 20.429006576538086 \u001b[0m(-8.796859741210938)\n",
            "     | > avg_postnet_loss:\u001b[92m 5.5633416175842285 \u001b[0m(-5.566458225250244)\n",
            "     | > avg_capaciton_reconstruction_loss:\u001b[92m 413216.125 \u001b[0m(-98668.0625)\n",
            "     | > avg_capacitron_vae_loss:\u001b[91m -0.00042412534821778536 \u001b[0m(+4.856527084484696e-06)\n",
            "     | > avg_capacitron_vae_beta_loss:\u001b[92m 44.516197204589844 \u001b[0m(-0.5097389221191406)\n",
            "     | > avg_capacitron_vae_kl_term:\u001b[91m 4.491630554199219 \u001b[0m(+0.23625946044921875)\n",
            "     | > avg_capacitron_beta:\u001b[92m 0.9781980514526367 \u001b[0m(-0.006090998649597168)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.15812799334526062 \u001b[0m(-0.1607477366924286)\n",
            "     | > avg_loss:\u001b[92m 26.150053024291992 \u001b[0m(-14.52406120300293)\n",
            "     | > avg_align_error:\u001b[91m 0.9835020024329424 \u001b[0m(+0.0028451010584831238)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000/best_model_20.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 5/25\u001b[0m\n",
            " --> /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-11-01 17:25:17) \u001b[0m\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 12.561102867126465  (12.561102867126465)\n",
            "     | > postnet_loss: 6.787648677825928  (6.787648677825928)\n",
            "     | > capaciton_reconstruction_loss: 304574.4375  (304574.4375)\n",
            "     | > capacitron_vae_loss: -0.0004142294928897172  (-0.0004142294928897172)\n",
            "     | > capacitron_vae_beta_loss: 43.4775276184082  (43.4775276184082)\n",
            "     | > capacitron_vae_kl_term: 5.264797210693359  (5.264797210693359)\n",
            "     | > capacitron_beta: 0.9718862175941467  (0.9718862175941467)\n",
            "     | > stopnet_loss: 0.11041544377803802  (0.11041544377803802)\n",
            "     | > loss: 19.458751678466797  (19.458751678466797)\n",
            "     | > align_error: 0.9848767872899771  (0.9848767872899771)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.7417483329772949 \u001b[0m(+0.07518219947814941)\n",
            "     | > avg_decoder_loss:\u001b[92m 12.561102867126465 \u001b[0m(-7.867903709411621)\n",
            "     | > avg_postnet_loss:\u001b[91m 6.787648677825928 \u001b[0m(+1.2243070602416992)\n",
            "     | > avg_capaciton_reconstruction_loss:\u001b[92m 304574.4375 \u001b[0m(-108641.6875)\n",
            "     | > avg_capacitron_vae_loss:\u001b[91m -0.0004142294928897172 \u001b[0m(+9.895855328068137e-06)\n",
            "     | > avg_capacitron_vae_beta_loss:\u001b[92m 43.4775276184082 \u001b[0m(-1.0386695861816406)\n",
            "     | > avg_capacitron_vae_kl_term:\u001b[91m 5.264797210693359 \u001b[0m(+0.7731666564941406)\n",
            "     | > avg_capacitron_beta:\u001b[92m 0.9718862175941467 \u001b[0m(-0.00631183385848999)\n",
            "     | > avg_stopnet_loss:\u001b[92m 0.11041544377803802 \u001b[0m(-0.047712549567222595)\n",
            "     | > avg_loss:\u001b[92m 19.458751678466797 \u001b[0m(-6.691301345825195)\n",
            "     | > avg_align_error:\u001b[91m 0.9848767872899771 \u001b[0m(+0.0013747848570346832)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000/best_model_24.pth\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: audio amplitude out of range, auto clipped.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 6/25\u001b[0m\n",
            " --> /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-11-01 17:26:07) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2023-11-01 17:26:26 -- STEP: 1/4 -- GLOBAL_STEP: 25\u001b[0m\n",
            "     | > decoder_loss: 10.671897888183594  (10.671897888183594)\n",
            "     | > postnet_loss: 10.251362800598145  (10.251362800598145)\n",
            "     | > capaciton_reconstruction_loss: 245835.296875  (245835.296875)\n",
            "     | > capacitron_vae_loss: -0.00041657095425762236  (-0.00041657095425762236)\n",
            "     | > capacitron_vae_beta_loss: 40.057464599609375  (40.057464599609375)\n",
            "     | > capacitron_vae_kl_term: 8.716302871704102  (8.716302871704102)\n",
            "     | > capacitron_beta: 0.9702973365783691  (0.9702973365783691)\n",
            "     | > stopnet_loss: 0.13627004623413086  (0.13627004623413086)\n",
            "     | > loss: 21.059114456176758  (21.059114456176758)\n",
            "     | > align_error: 0.9840188752859831  (0.9840188752859831)\n",
            "     | > grad_norm: 0  (0.0)\n",
            "     | > current_lr: 0.001 \n",
            "     | > step_time: 3.9072  (3.907191514968872)\n",
            "     | > loader_time: 0.0075  (0.007513999938964844)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > decoder_loss: 7.412466049194336  (7.412466049194336)\n",
            "     | > postnet_loss: 10.722015380859375  (10.722015380859375)\n",
            "     | > capaciton_reconstruction_loss: 220805.15625  (220805.15625)\n",
            "     | > capacitron_vae_loss: -0.00039297674084082246  (-0.00039297674084082246)\n",
            "     | > capacitron_vae_beta_loss: 41.2468376159668  (41.2468376159668)\n",
            "     | > capacitron_vae_kl_term: 7.2814178466796875  (7.2814178466796875)\n",
            "     | > capacitron_beta: 0.9655479192733765  (0.9655479192733765)\n",
            "     | > stopnet_loss: 0.12169839441776276  (0.12169839441776276)\n",
            "     | > loss: 18.255786895751953  (18.255786895751953)\n",
            "     | > align_error: 0.9852030724287033  (0.9852030724287033)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: audio amplitude out of range, auto clipped.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " ! Run is kept in /content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/Capacitron-Tacotron2-November-01-2023_05+21PM-0000000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\", line 1808, in fit\n",
            "    self._fit()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\", line 1764, in _fit\n",
            "    self.test_run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\", line 1680, in test_run\n",
            "    test_outputs = self.model.test_run(self.training_assets)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/models/base_tacotron.py\", line 157, in test_run\n",
            "    outputs_dict = synthesis(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/utils/synthesis.py\", line 221, in synthesis\n",
            "    outputs = run_model_torch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/utils/synthesis.py\", line 53, in run_model_torch\n",
            "    outputs = _func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/models/tacotron2.py\", line 290, in inference\n",
            "    decoder_outputs, alignments, stop_tokens = self.decoder.inference(encoder_outputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/layers/tacotron/tacotron2.py\", line 350, in inference\n",
            "    decoder_output, alignment, stop_token = self.decode(memory)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/layers/tacotron/tacotron2.py\", line 277, in decode\n",
            "    self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\", line 1347, in forward\n",
            "    ret = _VF.lstm_cell(\n",
            "RuntimeError: input has inconsistent input_size: got 1664 expected 2176\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-7-d1eedfb69ab2>\", line 119, in <cell line: 119>\n",
            "    trainer.fit()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\", line 1837, in fit\n",
            "    sys.exit(1)\n",
            "SystemExit: 1\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\", line 1808, in fit\n",
            "    self._fit()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\", line 1764, in _fit\n",
            "    self.test_run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\", line 1680, in test_run\n",
            "    test_outputs = self.model.test_run(self.training_assets)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/models/base_tacotron.py\", line 157, in test_run\n",
            "    outputs_dict = synthesis(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/utils/synthesis.py\", line 221, in synthesis\n",
            "    outputs = run_model_torch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/utils/synthesis.py\", line 53, in run_model_torch\n",
            "    outputs = _func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/models/tacotron2.py\", line 290, in inference\n",
            "    decoder_outputs, alignments, stop_tokens = self.decoder.inference(encoder_outputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/layers/tacotron/tacotron2.py\", line 350, in inference\n",
            "    decoder_output, alignment, stop_token = self.decode(memory)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/TTS/tts/layers/tacotron/tacotron2.py\", line 277, in decode\n",
            "    self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\", line 1347, in forward\n",
            "    ret = _VF.lstm_cell(\n",
            "RuntimeError: input has inconsistent input_size: got 1664 expected 2176\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1808\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1809\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1763\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_delay_epochs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\u001b[0m in \u001b[0;36mtest_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1679\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1680\u001b[0;31m                 \u001b[0mtest_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_assets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1681\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misimplemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_gpus\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misimplemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/TTS/tts/models/base_tacotron.py\u001b[0m in \u001b[0;36mtest_run\u001b[0;34m(self, assets)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             outputs_dict = synthesis(\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/TTS/tts/utils/synthesis.py\u001b[0m in \u001b[0;36msynthesis\u001b[0;34m(model, text, CONFIG, use_cuda, speaker_id, style_wav, style_text, use_griffin_lim, do_trim_silence, d_vector, language_id)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# synthesize voice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     outputs = run_model_torch(\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/TTS/tts/utils/synthesis.py\u001b[0m in \u001b[0;36mrun_model_torch\u001b[0;34m(model, inputs, speaker_id, style_mel, style_text, d_vector, language_id)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     outputs = _func(\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/TTS/tts/models/tacotron2.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, text, aux_input)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mpostnet_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/TTS/tts/layers/tacotron/tacotron2.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mstop_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/TTS/tts/layers/tacotron/tacotron2.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# self.decoder_hidden and self.decoder_cell: B x D_decoder_rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mdecoder_rnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         ret = _VF.lstm_cell(\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input has inconsistent input_size: got 1664 expected 2176",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d1eedfb69ab2>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1836\u001b[0m             \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 1",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from trainer import Trainer, TrainerArgs\n",
        "\n",
        "from TTS.config.shared_configs import BaseAudioConfig\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
        "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.models.tacotron2 import Tacotron2\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.tts.utils.speakers import SpeakerManager\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/\"\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Software_proj/Tacatron/tts_train_dir/\"\n",
        "\n",
        "# Using LJSpeech like dataset processing for the blizzard dataset\n",
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\",\n",
        "    meta_file_train=\"train.csv\",\n",
        "    path=data_path,\n",
        ")\n",
        "\n",
        "audio_config = BaseAudioConfig(\n",
        "    sample_rate=24000,\n",
        "    do_trim_silence=True,\n",
        "    trim_db=60.0,\n",
        "    signal_norm=False,\n",
        "    mel_fmin=0.0,\n",
        "    mel_fmax=11025,\n",
        "    spec_gain=1.0,\n",
        "    log_func=\"np.log\",\n",
        "    ref_level_db=20,\n",
        "    preemphasis=0.0,\n",
        ")\n",
        "\n",
        "# Using the standard Capacitron config\n",
        "capacitron_config = CapacitronVAEConfig(capacitron_VAE_loss_alpha=1.0, capacitron_capacity=50)\n",
        "\n",
        "config = Tacotron2Config(\n",
        "    run_name=\"Capacitron-Tacotron2\",\n",
        "    audio=audio_config,\n",
        "    capacitron_vae=capacitron_config,\n",
        "    use_capacitron_vae=True,\n",
        "    batch_size=32,  # Tune this to your gpu\n",
        "    max_audio_len=40 * 24000,  # Tune this to your gpu\n",
        "    min_audio_len=2 * 24000,\n",
        "    eval_batch_size=4,\n",
        "    num_loader_workers=2,\n",
        "    num_eval_loader_workers=2,\n",
        "    precompute_num_workers=2,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=6,\n",
        "    ga_alpha=0.0,\n",
        "    r=2,\n",
        "    optimizer=\"CapacitronOptimizer\",\n",
        "    optimizer_params={\"RAdam\": {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6}, \"SGD\": {\"lr\": 1e-5, \"momentum\": 0.9}},\n",
        "    attention_type=\"dynamic_convolution\",\n",
        "    grad_clip=0.0,  # Important! We overwrite the standard grad_clip with capacitron_grad_clip\n",
        "    double_decoder_consistency=False,\n",
        "    epochs=25,\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"pl\",\n",
        "    phonemizer=\"espeak\",\n",
        "    phoneme_cache_path=os.path.join(data_path, \"phoneme_cache3\"),\n",
        "    stopnet_pos_weight=15,\n",
        "    print_step=25,\n",
        "    print_eval=True,\n",
        "    mixed_precision=False,\n",
        "    seq_len_norm=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    use_speaker_embedding=True,\n",
        "    lr=1e-3,\n",
        "    lr_scheduler=\"StepwiseGradualLR\",\n",
        "    lr_scheduler_params={\n",
        "        \"gradual_learning_rates\": [\n",
        "            [0, 1e-3],\n",
        "            [2e4, 5e-4],\n",
        "            [4e5, 3e-4],\n",
        "            [6e4, 1e-4],\n",
        "            [8e4, 5e-5],\n",
        "        ]\n",
        "    },\n",
        "    scheduler_after_epoch=False,  # scheduler doesn't work without this flag\n",
        "    # Need to experiment with these below for capacitron\n",
        "    loss_masking=False,\n",
        "    decoder_loss_alpha=1.0,\n",
        "    postnet_loss_alpha=1.0,\n",
        "    postnet_diff_spec_alpha=0.0,\n",
        "    decoder_diff_spec_alpha=0.0,\n",
        "    decoder_ssim_alpha=0.0,\n",
        "    postnet_ssim_alpha=0.0,\n",
        ")\n",
        "\n",
        "ap = AudioProcessor(**config.audio.to_dict())\n",
        "\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "\n",
        "train_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\n",
        "\n",
        "speaker_manager = SpeakerManager()\n",
        "speaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\n",
        "config.num_speakers = speaker_manager.num_speakers\n",
        "\n",
        "model = Tacotron2(config, ap, tokenizer, speaker_manager=speaker_manager)\n",
        "\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(),\n",
        "    config,\n",
        "    output_path,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        "    training_assets={\"audio_processor\": ap},\n",
        ")\n",
        "\n",
        "trainer.fit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
