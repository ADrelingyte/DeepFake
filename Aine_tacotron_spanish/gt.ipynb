{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:24000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:11025\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60.0\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " | > Found 50 files in /srv/storage/idmctal@storage1.nancy.grid5000.fr/2023/m2/adrelingyte/data/spanish/cml_tts_dataset_spanish_v0.1\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": " [!] You do not have enough samples for the evaluation set. You can work around this setting the 'eval_split_size' parameter to a minimum of 0.02",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/adrelingyte/workspace/DeepFake/Aine_tacotron_spanish/gt.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnancy.g5k/home/adrelingyte/workspace/DeepFake/Aine_tacotron_spanish/gt.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m ap \u001b[39m=\u001b[39m AudioProcessor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mto_dict())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnancy.g5k/home/adrelingyte/workspace/DeepFake/Aine_tacotron_spanish/gt.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m tokenizer, config \u001b[39m=\u001b[39m TTSTokenizer\u001b[39m.\u001b[39minit_from_config(config)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bnancy.g5k/home/adrelingyte/workspace/DeepFake/Aine_tacotron_spanish/gt.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m train_samples, eval_samples \u001b[39m=\u001b[39m load_tts_samples(dataset_config, eval_split\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnancy.g5k/home/adrelingyte/workspace/DeepFake/Aine_tacotron_spanish/gt.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m speaker_manager \u001b[39m=\u001b[39m SpeakerManager()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnancy.g5k/home/adrelingyte/workspace/DeepFake/Aine_tacotron_spanish/gt.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m speaker_manager\u001b[39m.\u001b[39mset_ids_from_data(train_samples \u001b[39m+\u001b[39m eval_samples, parse_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mspeaker_name\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/TTS/TTS/tts/datasets/__init__.py:133\u001b[0m, in \u001b[0;36mload_tts_samples\u001b[0;34m(datasets, eval_split, formatter, eval_split_max_size, eval_split_size)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         eval_size_per_dataset \u001b[39m=\u001b[39m eval_split_max_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(datasets) \u001b[39mif\u001b[39;00m eval_split_max_size \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m         meta_data_eval, meta_data_train \u001b[39m=\u001b[39m split_dataset(meta_data_train, eval_size_per_dataset, eval_split_size)\n\u001b[1;32m    134\u001b[0m     meta_data_eval_all \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m meta_data_eval\n\u001b[1;32m    135\u001b[0m meta_data_train_all \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m meta_data_train\n",
      "File \u001b[0;32m~/workspace/TTS/TTS/tts/datasets/__init__.py:37\u001b[0m, in \u001b[0;36msplit_dataset\u001b[0;34m(items, eval_split_max_size, eval_split_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         eval_split_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(items) \u001b[39m*\u001b[39m eval_split_size)\n\u001b[0;32m---> 37\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     eval_split_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     39\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39m [!] You do not have enough samples for the evaluation set. You can work around this setting the \u001b[39m\u001b[39m'\u001b[39m\u001b[39meval_split_size\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameter to a minimum of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     40\u001b[0m     \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(items)\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m0\u001b[39m)\n\u001b[1;32m     43\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mshuffle(items)\n",
      "\u001b[0;31mAssertionError\u001b[0m:  [!] You do not have enough samples for the evaluation set. You can work around this setting the 'eval_split_size' parameter to a minimum of 0.02"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from trainer import Trainer, TrainerArgs\n",
    "import gruut\n",
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.config.shared_configs import BaseAudioConfig\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig, CapacitronVAEConfig\n",
    "from TTS.tts.configs.fast_speech_config import FastSpeechConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.forward_tts import ForwardTTS\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "\n",
    "'''CHANGED FORMATTER TO BE USING THE FIRST 100 ROWS OF DATA'''\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "file_output = \"/srv/storage/idmctal@storage1.nancy.grid5000.fr/2023/m2/adrelingyte/data/spanish/cml_tts_dataset_spanish_v0.1/tmp\"\n",
    "\n",
    "output_path = \"/srv/storage/idmctal@storage1.nancy.grid5000.fr/2023/m2/adrelingyte/data/spanish/cml_tts_dataset_spanish_v0.1\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Create a BaseDatasetConfig object\n",
    "dataset_config = BaseDatasetConfig(formatter=\"custom_formatter2\", meta_file_train=\"train.csv\", path=os.path.join(output_path))\n",
    "\n",
    "\n",
    "# Load the dataset using your custom formatter\n",
    "\n",
    "audio_config = BaseAudioConfig(\n",
    "    sample_rate=24000,\n",
    "    do_trim_silence=True,\n",
    "    trim_db=60.0,\n",
    "    signal_norm=False,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=11025,\n",
    "    spec_gain=1.0,\n",
    "    log_func=\"np.log\",\n",
    "    ref_level_db=20,\n",
    "    preemphasis=0.0,\n",
    ")\n",
    "\n",
    "# Using the standard Capacitron config\n",
    "capacitron_config = CapacitronVAEConfig(capacitron_VAE_loss_alpha=1.0, capacitron_capacity=50)\n",
    "\n",
    "config = Tacotron2Config(\n",
    "    run_name=\"Capacitron-Tacotron2\",\n",
    "    audio=audio_config,\n",
    "    capacitron_vae=capacitron_config,\n",
    "    use_capacitron_vae=True,\n",
    "    batch_size=32,  # Tune this to your gpu\n",
    "    max_audio_len=40 * 24000,  # Tune this to your gpu\n",
    "    min_audio_len=2 * 24000,\n",
    "    eval_batch_size=4,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    precompute_num_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=6,\n",
    "    ga_alpha=0.0,\n",
    "    r=2,\n",
    "    optimizer=\"CapacitronOptimizer\",\n",
    "    optimizer_params={\"RAdam\": {\"betas\": [0.9, 0.998], \"weight_decay\": 1e-6}, \"SGD\": {\"lr\": 1e-5, \"momentum\": 0.9}},\n",
    "    attention_type=\"dynamic_convolution\",\n",
    "    grad_clip=0.0,  # Important! We overwrite the standard grad_clip with capacitron_grad_clip\n",
    "    double_decoder_consistency=False,\n",
    "    epochs=25,\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"es\",\n",
    "    phonemizer=\"gruut\",\n",
    "    phoneme_cache_path=os.path.join(file_output, \"phoneme_cache3\"),\n",
    "    stopnet_pos_weight=15,\n",
    "    print_step=25,\n",
    "    print_eval=True,\n",
    "    mixed_precision=False,\n",
    "    seq_len_norm=False,\n",
    "    output_path=file_output,\n",
    "    datasets=[dataset_config],\n",
    "    use_speaker_embedding=True,\n",
    "    lr=1e-3,\n",
    "    lr_scheduler=\"StepwiseGradualLR\",\n",
    "    lr_scheduler_params={\n",
    "        \"gradual_learning_rates\": [\n",
    "            [0, 1e-3],\n",
    "            [2e4, 5e-4],\n",
    "            [4e5, 3e-4],\n",
    "            [6e4, 1e-4],\n",
    "            [8e4, 5e-5],\n",
    "        ]\n",
    "    },\n",
    "    scheduler_after_epoch=False,  # scheduler doesn't work without this flag\n",
    "    # Need to experiment with these below for capacitron\n",
    "    loss_masking=False,\n",
    "    decoder_loss_alpha=1.0,\n",
    "    postnet_loss_alpha=1.0,\n",
    "    postnet_diff_spec_alpha=0.0,\n",
    "    decoder_diff_spec_alpha=0.0,\n",
    "    decoder_ssim_alpha=0.0,\n",
    "    postnet_ssim_alpha=0.0,\n",
    ")\n",
    "\n",
    "ap = AudioProcessor(**config.audio.to_dict())\n",
    "\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "train_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\n",
    "\n",
    "speaker_manager = SpeakerManager()\n",
    "speaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\n",
    "config.num_speakers = speaker_manager.num_speakers\n",
    "\n",
    "model = Tacotron2(config, ap, tokenizer, speaker_manager=speaker_manager)\n",
    "\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(),\n",
    "    config,\n",
    "    file_output,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    "    training_assets={\"audio_processor\": ap},\n",
    ")\n",
    "\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
